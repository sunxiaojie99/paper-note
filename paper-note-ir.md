## ir-gengerate

**[Transformer Memory as a Differentiable Search Index](https://arxiv.org/abs/2202.06991). *Tay et al.*, Arxiv 2022. [[Video](https://www.youtube.com/watch?v=qlB0TPBQ7YY)] (DSI) google**

èƒ½å¦å–ä»£retrievalï¼Ÿç”Ÿæˆçš„æ–‡æ¡£æ•°é‡æœ‰è¦æ±‚å—ï¼Ÿèƒ½å¤ªå¤šå—

æœ€å¤šåˆ°320kæ–‡æ¡£ï¼Œ32wï¼Ÿ

Indexingï¼ˆinput: document tokens, output: identifiersï¼‰+Retrievalï¼ˆinput: query, output:  a ranked list of candidate docidsï¼‰

**a DSI model** 

- d->did: trained to index a corpus of documents
- q->did (train) : optionally fine-tune on an available set of labeled data (queries and labeled documents)
- q->did: used to retrieve relevant documentsâ€”all within a single, unified model.

options

1. what dï¼Ÿè¢«ç¼–ç çš„å†…å®¹

   - ğŸŒŸ <u>Direct Indexing</u>ï¼šfirst L tokens of a document

   - <u>Set Indexing</u>: de-duplicates repeated terms + removes stopwords + first L tokens of a document

   - <u>Inverted Index</u>: randomly subsample a single contiguous chunk of k tokens

     <img src="paper-note-ir.assets/image-20230224141148967.png" alt="image-20230224141148967" style="zoom:43%;" />

2. what did?

   - <u>Unstructured Atomic Identifiers</u>ï¼šassign each an arbitrary (and possibly random) unique integer identifierï¼Œç”Ÿæˆä¸€æ¬¡å°±å¯ä»¥ç”Ÿæˆå‡ºæ¥ã€‚åœ¨ä¸€ä¸ªè¯è¡¨ä¸Šæ‰§è¡Œsoftmaxã€‚ï¼ˆä¸é€‚é…æ–°æ–‡æ¡£åŠ å…¥ï¼Ÿï¼‰

   - <u>Naively Structured String Identifiers</u> + beam searchï¼š treats unstructured identifiers as tokenizable strings

   - ğŸŒŸ<u>Semantically Structured Identifiers</u>+ beam searchï¼šéœ€è¦æ»¡è¶³ï¼ˆ1ï¼‰the docid should capture some information about the semantics of its associated documentï¼›ï¼ˆ2ï¼‰ the docid should be structured in a way that the search space is effectively reduced after each decoding stepï¼›

     - a fully unsupervised pre-processingï¼ˆå¯¹bertç¼–ç çš„doc embedding å±‚æ¬¡èšç±»ï¼‰ -> a fully end-to-end manner(further)

       <img src="paper-note-ir.assets/image-20230224140720545.png" alt="image-20230224140720545" style="zoom:50%;" />

3. how to indexï¼Ÿ

   - ğŸŒŸInputs2Targetï¼šdoc_tokens -> docid.
   - Targets2Inputsï¼šdocid -> doc_tokens
   - Bidirectionalï¼š trains both Inputs2Targets and Targets2Inputs within the same co-training setupï¼ˆwith prefix token ï¼‰
   - Span Corruption

4. how to train

   - two-stage ( indexing + fine-tuning: map queries to docids)
   - ğŸŒŸ co-train (multi-task learning)ï¼š (e.g., using task prompts to differentiate them).

<img src="paper-note-ir.assets/image-20230224141219686.png" alt="image-20230224141219686" style="zoom:30%;" />