## IR

[awesome-pretrained-models-for-information-retrieval](https://github.com/ict-bigdatalab/awesome-pretrained-models-for-information-retrieval)

[awesome-generative-retrieval-models](https://github.com/Chriskuei/awesome-generative-retrieval-models) 

- 

- [CorpusBrain: Pre-train a Generative Retrieval Model for Knowledge-Intensive Language Tasks](https://arxiv.org/abs/2208.07652). *Chen et al.*, CIKM 2022. [[Code](https://github.com/ict-bigdatalab/CorpusBrain)] 
- [A Neural Corpus Indexer for Document Retrieval](https://arxiv.org/abs/2206.02743). *Wang et al.*, Arxiv 2022. (**NCI**)
- [Autoregressive Entity Retrieval](https://arxiv.org/pdf/2010.00904.pdf). *Cao et al*, ICLR 2021. [[Code](https://github.com/facebookresearch/GENRE)] (**GENRE**)
- [DynamicRetriever: A Pre-training Model-based IR System with Neither Sparse nor Dense Index](https://arxiv.org/pdf/2203.00537.pdf). *Zhou et al*, Arxiv 2022. (**DynamicRetriever**)

[Semantic-Retrieval-Models](https://github.com/caiyinqiong/Semantic-Retrieval-Models) 

- [Less is More: Pre-training a Strong Siamese Encoder Using a Weak Decoder](https://arxiv.org/pdf/2102.09206.pdf)（Lu et al., 2021, EMNLP, **SEED-Encoder**）
- [LexMAE: Lexicon-Bottlenecked Pretraining for Large-Scale Retrieval](https://arxiv.org/pdf/2208.14754v1.pdf)（Shen et al., 2022, arXiv）

- [SimLM: Pre-training with representation bottleneck for dense passage retrieval](https://arxiv.org/pdf/2207.02578.pdf)（Wang et al., 2022, arXiv）

- [ConTextual Mask Auto-Encoder for Dense Passage Retrieval](https://arxiv.org/pdf/2208.07670.pdf)（Wu et al., 2022, arXiv）
- [RetroMAE: Pre-Training Retrieval-oriented Language Models Via Masked Auto-Encoder](https://arxiv.org/pdf/2205.12035.pdf)（Xiao and Liu et al., 2022, EMNLP)
- [Retrieval Oriented Masking Pre-training Language Model for Dense Passage Retrieval](https://arxiv.org/pdf/2210.15133.pdf)（Long et al., 2022, arXiv）



## NLP

## lm

AUTOMATIC CHAIN OF THOUGHT PROMPTING IN LARGE LANGUAGE MODELS

Challenging BIG-Bench tasks and whether chain-of-thought can solve them

On the Advance of Making Language Models Better Reasoners

Evaluating Large Language Models Trained on Code



Transformer Feed-Forward Layers Are Key-Value Memories







[Cut your Losses with Squentropy](https://arxiv.org/abs/2302.03952)

[Protecting Language Generation Models via Invisible Watermarking](https://arxiv.org/abs/2302.03162)

[ResMem- Learn what you can and memorize the rest](https://arxiv.org/abs/2302.01576)





[Big Little Transformer Decoder](https://arxiv.org/abs/2302.07863)

[Deep Transformers without Shortcuts- Modifying Self-Attention for Faithful Signal Propagation](https://arxiv.org/abs/2302.10322) 



[Hyena Hierarchy- Towards Larger Convolutional Language Models](https://arxiv.org/abs/2302.10866) 



Why are Adaptive Methods Good for Attention Models？